{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nemes\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Nemes\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\Nemes\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Nemes\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers as layers\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Activation, Dense, Conv1D, Conv2D, Dropout, Flatten, MaxPooling2D, BatchNormalization, GlobalMaxPooling1D\n",
    "from tensorflow.keras import optimizers\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils.augment import noise_augment, shift_augment, apply_augment\n",
    "from utils.deep import preProcess, smoothLabels, oneHot, applyOneHot, evaluate_set\n",
    "from utils.visualize import showMe, showHistory\n",
    "from config.default import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for train for class Chew\n",
      "No data available for train for class Chew\n",
      "No data available for train for class Chew\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "No data available for train for class Smile\n",
      "50 sessions loaded for training\n",
      "8 sessions loaded for validation\n"
     ]
    }
   ],
   "source": [
    "root_path = 'C:/resources/EMG/'\n",
    "post_fix = '_1s_cleaned' #'_1s_new' #\n",
    "classes = settings['classes']\n",
    "\n",
    "\n",
    "sessions_to_val = []#'session_4'] # ['session_1','session_2','session_3','session_4']    #[] # \n",
    "subject_to_val = ['S001',  'S105']\n",
    "include = ['S002', 'S004', 'S005', 'S006', 'S007', 'S008', 'S009', 'S101', 'S102']   #['S101', 'S102'] #\n",
    "exclude = ['S003']\n",
    "# use session4 for validation\n",
    "train_sessions = []\n",
    "val_sessions = []\n",
    "for subject in os.listdir(root_path):\n",
    "    if subject not in exclude:\n",
    "    #if subject in include:\n",
    "        for session in os.listdir(os.path.join(root_path,subject)):\n",
    "            if session in sessions_to_val or subject in subject_to_val:\n",
    "                val_sessions.append(os.path.join(root_path,subject, session))\n",
    "            else:\n",
    "                train_sessions.append(os.path.join(root_path,subject, session))\n",
    "        #print(f\"{len(os.listdir(os.path.join(root_path,subject)))} session loaded from subject: {subject}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_records = {}\n",
    "if len(train_sessions) > 0:\n",
    "    for c in classes:\n",
    "        class_data = []\n",
    "        for session in train_sessions:\n",
    "            data = np.load(os.path.join(session,c+post_fix+'.npy'),allow_pickle=True)\n",
    "            if data.shape[0] != 0:\n",
    "                class_data.append(data)\n",
    "            else:\n",
    "                #val_records[c] = np.random.rand(1, 4, 500)\n",
    "                #print(f'WARNING! CREATING RANDOM DATA FOR {c}')\n",
    "                print(f\"No data available for train for class {c}\")\n",
    "        \n",
    "        train_records[c] = np.concatenate(class_data)\n",
    "    print(f\"{len(train_sessions)} sessions loaded for training\")\n",
    "else:\n",
    "    print(\"No train session available\")\n",
    "\n",
    "val_records = {}\n",
    "for c in classes:\n",
    "    class_data = []\n",
    "    for session in val_sessions:\n",
    "        data = np.load(os.path.join(session,c+post_fix+'.npy'),allow_pickle=True)\n",
    "        if data.shape[0] != 0:\n",
    "            class_data.append(data)\n",
    "    if len(class_data) != 0:\n",
    "        val_records[c] = np.concatenate(class_data)\n",
    "    else:\n",
    "        #val_records[c] = np.random.rand(1, 4, 500)\n",
    "        #print(f'WARNING! CREATING RANDOM DATA FOR {c}')\n",
    "        print(f\"No data available for validation for class {c}\")\n",
    "\n",
    "print(f\"{len(val_sessions)} sessions loaded for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(X):\n",
    "    y = []\n",
    "    for i, r in enumerate(X):\n",
    "        l = np.ones(X[r].shape[0])*i\n",
    "        y = y + l.tolist()\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SET\n",
      "Rest -> (2088, 4, 500)\n",
      "Eyebrow -> (2880, 4, 500)\n",
      "Chew -> (1866, 4, 500)\n",
      "Smile -> (1215, 4, 500)\n",
      "VAL SET\n",
      "Rest -> (201, 4, 500)\n",
      "Eyebrow -> (519, 4, 500)\n",
      "Chew -> (294, 4, 500)\n",
      "Smile -> (285, 4, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN SET\")\n",
    "for r in train_records:\n",
    "    print(f'{r} -> {train_records[r].shape}')\n",
    "\n",
    "print(\"VAL SET\")\n",
    "for r in val_records:\n",
    "    print(f'{r} -> {val_records[r].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "(8049, 4, 500)\n",
      "(8049,)\n",
      "Validation:\n",
      "(1299, 4, 500)\n",
      "(1299,)\n"
     ]
    }
   ],
   "source": [
    "n_channels = train_records[\"Rest\"].shape[1]\n",
    "input_length = train_records[\"Rest\"].shape[2]\n",
    "\n",
    "\n",
    "print('Train')\n",
    "train_y = create_labels(train_records)\n",
    "train_X = np.concatenate((list(train_records.values())), axis=0)\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "\n",
    "print('Validation:')\n",
    "val_y = create_labels(val_records)\n",
    "val_X = np.concatenate((list(val_records.values())), axis=0)\n",
    "print(val_X.shape)\n",
    "print(val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(train_X, train_y))\n",
    "random.seed(42)\n",
    "random.shuffle(c)\n",
    "train_X,train_y = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8049, 4, 500)\n",
      "(8049, 4)\n",
      "(1299, 4, 500)\n",
      "(1299, 4)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(train_X).reshape(-1,n_channels,input_length)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_X = np.array(val_X).reshape(-1,n_channels,input_length)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "\n",
    "train_y = applyOneHot(train_y,len(classes))\n",
    "val_y = applyOneHot(val_y,len(classes))\n",
    "\n",
    "\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "\n",
    "print(val_X.shape)\n",
    "print(val_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = apply_augment(train_X, train_y)\n",
    "print(\"After augmentation\")\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dense_model():\n",
    "    inspected_chanels= train_X.shape[1]\n",
    "    signal_length=     train_X.shape[2]\n",
    "    input_layer = keras.Input(shape = (inspected_chanels,signal_length), name='input')\n",
    "    x = layers.Flatten()(input_layer)\n",
    "\n",
    "\n",
    "    l2 = 0.001\n",
    "    x     = layers.Dense(200,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x     = layers.Dense(100,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "\n",
    "    x     = layers.Dense(50,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "    x     = layers.Dropout(.2)(x)\n",
    "\n",
    "\n",
    "    x     = layers.Dense(30,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "\n",
    "\n",
    "\n",
    "    x     = layers.Dense(10,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "    x     = layers.Dropout(.3)(x)\n",
    "\n",
    "\n",
    "    output = layers.Dense(3, activation='softmax')(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inspected_chanels= train_X.shape[1]\n",
    "    input_length=     train_X.shape[2]\n",
    "    l2 = 0.00001\n",
    "\n",
    "    input_layer = keras.Input(shape = (inspected_chanels,input_length,1), name='input')\n",
    "\n",
    "    x     = layers.AveragePooling2D(pool_size=(1,5))(input_layer) # resample\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "    x     = layers.Conv2D(256, kernel_size=(1,5), padding='same', activation='relu', kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "    x     = layers.AveragePooling2D(pool_size=(1,5))(x)\n",
    "\n",
    "    x     = layers.Conv2D(256, kernel_size=(4,1), padding='same', activation='relu', kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "    x     = layers.AveragePooling2D(pool_size=(4,1))(x)\n",
    "\n",
    "    x     = layers.Dense(500,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.Flatten()(x)\n",
    "\n",
    " \n",
    "   \n",
    "    \n",
    "\n",
    "    x     = layers.Dense(200,kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x     = layers.BatchNormalization()(x)\n",
    "    x     = layers.Dropout(.2)(x)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    output = layers.Dense(len(classes), activation='softmax')(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 4, 500, 1)]       0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_6 (Average (None, 4, 100, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 100, 256)       1536      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 4, 100, 256)       1024      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 4, 20, 256)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 4, 20, 256)        262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 20, 256)        1024      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 1, 20, 256)        0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1, 20, 500)        128500    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 200)               2000200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 804       \n",
      "=================================================================\n",
      "Total params: 2,396,288\n",
      "Trainable params: 2,394,864\n",
      "Non-trainable params: 1,424\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#model = get_dense_model()\n",
    "model = get_model()\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                               patience=3, min_lr=1e-7, min_delta = 0.0001)\n",
    "                              \n",
    "\n",
    "# early_stopping =  tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.000001)\n",
    "\n",
    "opt = keras.optimizers.SGD(learning_rate=1e-5) #, momentum=0.9)\n",
    "model.compile(optimizer=opt,\n",
    "          loss='categorical_crossentropy',\n",
    "          metrics=['categorical_accuracy']\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.7111 - categorical_accuracy: 0.7958 - val_loss: 0.5324 - val_categorical_accuracy: 0.8984\n",
      "Epoch 2/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6908 - categorical_accuracy: 0.8118 - val_loss: 0.5291 - val_categorical_accuracy: 0.8899\n",
      "Epoch 3/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.7013 - categorical_accuracy: 0.8016 - val_loss: 0.5319 - val_categorical_accuracy: 0.8899\n",
      "Epoch 4/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6942 - categorical_accuracy: 0.8107 - val_loss: 0.5234 - val_categorical_accuracy: 0.9030\n",
      "Epoch 5/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6832 - categorical_accuracy: 0.8084 - val_loss: 0.5086 - val_categorical_accuracy: 0.9107\n",
      "Epoch 6/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6808 - categorical_accuracy: 0.8066 - val_loss: 0.5122 - val_categorical_accuracy: 0.9130\n",
      "Epoch 7/50\n",
      "2013/2013 [==============================] - 13s 6ms/step - loss: 0.6810 - categorical_accuracy: 0.8159 - val_loss: 0.5352 - val_categorical_accuracy: 0.9022\n",
      "Epoch 8/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6677 - categorical_accuracy: 0.8163 - val_loss: 0.5074 - val_categorical_accuracy: 0.9161\n",
      "Epoch 9/50\n",
      "2013/2013 [==============================] - 13s 6ms/step - loss: 0.6790 - categorical_accuracy: 0.8141 - val_loss: 0.5232 - val_categorical_accuracy: 0.8999\n",
      "Epoch 10/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6694 - categorical_accuracy: 0.8253 - val_loss: 0.5111 - val_categorical_accuracy: 0.9030\n",
      "Epoch 11/50\n",
      "2013/2013 [==============================] - 13s 6ms/step - loss: 0.6811 - categorical_accuracy: 0.8153 - val_loss: 0.5030 - val_categorical_accuracy: 0.9138\n",
      "Epoch 12/50\n",
      "2013/2013 [==============================] - 13s 6ms/step - loss: 0.6611 - categorical_accuracy: 0.8238 - val_loss: 0.5274 - val_categorical_accuracy: 0.9007\n",
      "Epoch 13/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6685 - categorical_accuracy: 0.8192 - val_loss: 0.5087 - val_categorical_accuracy: 0.9145\n",
      "Epoch 14/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6504 - categorical_accuracy: 0.8247 - val_loss: 0.5309 - val_categorical_accuracy: 0.9007\n",
      "Epoch 15/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6516 - categorical_accuracy: 0.8308 - val_loss: 0.5018 - val_categorical_accuracy: 0.9076\n",
      "Epoch 16/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6461 - categorical_accuracy: 0.8264 - val_loss: 0.5036 - val_categorical_accuracy: 0.9130\n",
      "Epoch 17/50\n",
      "2013/2013 [==============================] - 13s 7ms/step - loss: 0.6501 - categorical_accuracy: 0.8322 - val_loss: 0.5021 - val_categorical_accuracy: 0.9130\n",
      "Epoch 18/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6489 - categorical_accuracy: 0.8252 - val_loss: 0.4919 - val_categorical_accuracy: 0.9138\n",
      "Epoch 19/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6458 - categorical_accuracy: 0.8295 - val_loss: 0.4917 - val_categorical_accuracy: 0.9192\n",
      "Epoch 20/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6586 - categorical_accuracy: 0.8246 - val_loss: 0.4967 - val_categorical_accuracy: 0.9138\n",
      "Epoch 21/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6548 - categorical_accuracy: 0.8246 - val_loss: 0.5038 - val_categorical_accuracy: 0.9145\n",
      "Epoch 22/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6417 - categorical_accuracy: 0.8328 - val_loss: 0.4945 - val_categorical_accuracy: 0.9192\n",
      "Epoch 23/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6559 - categorical_accuracy: 0.8263 - val_loss: 0.5023 - val_categorical_accuracy: 0.9192\n",
      "Epoch 24/50\n",
      "2013/2013 [==============================] - 14s 7ms/step - loss: 0.6448 - categorical_accuracy: 0.8327 - val_loss: 0.4957 - val_categorical_accuracy: 0.9145\n",
      "Epoch 25/50\n",
      "2011/2013 [============================>.] - ETA: 0s - loss: 0.6456 - categorical_accuracy: 0.8320"
     ]
    }
   ],
   "source": [
    "##############################################\n",
    "############     TRAIN MODEL     #############\n",
    "##############################################\n",
    "\n",
    "batch_size = 4\n",
    "history = model.fit(train_X,\n",
    "                    train_y,\n",
    "                    validation_data=(val_X, val_y),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=50,\n",
    "                    shuffle=True,\n",
    "                    callbacks = [reduce_lr]\n",
    "                    )\n",
    "\n",
    "showHistory(history)      \n",
    "try:    \n",
    "    acc = max(history.history['val_accuracy'])\n",
    "except:\n",
    "    acc = max(history.history['val_categorical_accuracy'])\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "#tf.config.run_functions_eagerly(True)\n",
    "\n",
    "for train, test in skf.split(X, y.argmax(1)):\n",
    "  \n",
    "    X_train = X[train]\n",
    "    X_test  = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test  = y[test]\n",
    "\n",
    "    #batch_size = 30 #len(X_train)\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "\n",
    "\n",
    "    tuner = kt.tuners.RandomSearch(\n",
    "        get_hyper_model,\n",
    "        objective='val_accuracy',\n",
    "        max_trials=100)\n",
    "\n",
    "    tuner.search(X_train,\n",
    "                y_train,\n",
    "                validation_data=(X_test, y_test),\n",
    "                batch_size=batch_size,\n",
    "                epochs=20,\n",
    "                shuffle=True)\n",
    "\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "                             \n",
    "    break\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679292b2ee7d4e5d8451f110b8f1b6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "No data for class Chew\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "No data for class Chew\n",
      "####################\n",
      "No data for class Chew\n",
      "No data for class Smile\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "####################\n",
      "No data for class Smile\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "Global accuracy: 88.3%\n",
      "          Accuracy\n",
      "Subject           \n",
      "S002     79.000000\n",
      "S004     91.500000\n",
      "S005     86.500000\n",
      "S006     72.750000\n",
      "S007     78.000000\n",
      "S008     94.750000\n",
      "S009     94.000000\n",
      "S010     94.000000\n",
      "S011     88.000000\n",
      "S101     96.000000\n",
      "S102     92.500000\n",
      "S103     93.333333\n",
      "S104     89.750000\n",
      "S106     80.250000\n"
     ]
    }
   ],
   "source": [
    "evaluate_set(model, train_sessions, classes, post_fix, input_length=input_length, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644678f6a34048ffa6ca0021f8a19caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "####################\n",
      "####################\n",
      "####################\n",
      "No data for class Chew\n",
      "####################\n",
      "No data for class Chew\n",
      "####################\n",
      "No data for class Rest\n",
      "####################\n",
      "No data for class Chew\n",
      "####################\n",
      "Global accuracy: 89.62%\n",
      "         Accuracy\n",
      "Subject          \n",
      "S001        91.25\n",
      "S105        88.00\n"
     ]
    }
   ],
   "source": [
    "evaluate_set(model, val_sessions, classes,  post_fix, input_length=input_length, log = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a81b55685ebb6380129efe90592a7e4f2f571da2ab32c8bbcf8b970d830ead19"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
